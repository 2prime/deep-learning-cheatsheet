\documentclass[pdftex,10pt,a4paper]{scrartcl}

\usepackage[a4paper,left=2.5cm,right=2.5cm,bottom=3cm,top=3cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{url}
\usepackage{todonotes}
\usepackage[numbers,sort]{natbib}
\parindent=0cm

\title{Graph Convolutions}
\date{\vspace{-5ex}}

\begin{document}

\maketitle

\section{Preliminaries}

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a \emph{weighted graph} with $\mathcal{V} = \{1, \ldots, n\}$ and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$.
Additionally, there exists an \emph{adjacency matrix} $\mathbf{W} \in \mathbb{R}^{n \times n}$, where $W_{ij} > 0$ iff $(i, j) \in \mathcal{E}$ and $W_{ij} = 0$ iff $(i, j) \not\in \mathcal{E}$.
Note that $\mathbf{W}$ is usually \emph{sparse} with $|\mathcal{E}| \ll n^2$ entries.
$\mathcal{G}$ is called \emph{undirected} iff $W_{ij} = W_{ji}$ for all $i,j \in \mathcal{V}$ and is called \emph{without self-loops} iff $W_{ii} = 0$ for all $i \in \mathcal{V}$.
\\\\

The \emph{unnormalized Laplacian} of an weighted undirected graph without self-loops is a $n \times n$ symmetric positive-semidefinite matrix $\mathbf{L} = \mathbf{D} - \mathbf{W}$, where $\mathbf{D} = \mathrm{diag}\left( \sum_{i \in \mathcal{V}} w_{ij}\right)$. (wrong syntax)
The \emph{normalized Laplacian}

$\mathcal{N}(i)$

A \emph{signal} $f \colon \mathcal{V} \to \mathbb{R}^m$ respectively $\mathbf{F} \in \mathbb{R}^{n \times m}$

Let $\mathbf{u} \colon \mathcal{V} \times \mathcal{V} \to \mathbb{R}^d$ define a $d$-dimensional vector of \emph{pseudo-coordinates}
e.g.\ the vector from $i$ to $j$ in euclidean space $\mathbf{u}(i,j) = \mathbf{p}(j) - \mathbf{p}(i)$ with \emph{graph embeddings} $\mathbf{p} \colon \mathcal{V} \to \mathbb{R}^d$
\cite{Bronstein2017}

\section{Convolutions}

\paragraph{MoNet}

MoNet~\cite{Monti2016} uses the generic \emph{patch operator} for a specific $k \in \{ 1, \ldots, K \}$
\begin{equation*}
  D_k(i)f = \sum_{j \in \mathcal{N}(i)} w_k(\mathbf{u}(i, j)) f(j)
\end{equation*}
for the spatial definition of a convolution on a graph or manifold signal $f \colon \mathcal{V} \to \mathbb{R}$ respectively $\mathbf{f} \in \mathbb{R}^n$
\begin{equation*}
  (\mathbf{f} \star \mathbf{g})(i) = \sum_k^K D_k(i)f
\end{equation*}
where $K \in \mathbb{N}$ is the dimensionality of the extracted patch and $\mathbf{w}(\mathbf{u}) \in \mathbb{R}^K$ with $\mathbf{w}(\mathbf{u}) = (w_1(\mathbf{u}), \ldots, w_K(\mathbf{u}))$ is a weighting function parametrized by some learnable parameters.
MoNet~\cite{Monti2016} suggests the use of a weighted \emph{gaussian kernel}
\begin{equation*}
  w_k(\mathbf{u}) = \exp \left(-\frac{1}{2} {(\mathbf{u} - \boldsymbol{\mu}_k)}^{\top} {\mathrm{diag}(\boldsymbol{\sigma}_k)}^{-1} (\mathbf{u} - \boldsymbol{\mu}_k) \right)
\end{equation*}

with the diagonal covariance matrix $\mathrm{diag}(\boldsymbol{\sigma}_k) \in \mathbb{R}^{d \times d}$ and mean vector $\boldsymbol{\mu}_k \in \mathbb{R}^d$, which results in $2Kd$ learnable parameters for the patch operator.

For arbitary graphs one can choose the pseudo-coordinates to use the degrees of the nodes $\mathbf{u}(i,j) = \left( \tfrac{1}{\sqrt{D_{ii}}}, \tfrac{1}{\sqrt{D_{jj}}} \right)$, whereas one can take the polar coordinates $\mathbf{u} = (\rho, \varphi)$ or $\mathbf{u} = (r, \varphi, \theta)$ for 2D respectively 3d graph embeddings like discrete manifolds.

\paragraph{SplineNet}

Let $\xi = (t_0, \ldots, t_)$ be a node vector.
An \emph{open b-spline-function} $N_k^m$ over $k \in \{ 1, \ldots, K \}$ is recursively defined by
\begin{equation*}
  N_k^0(t) = \begin{cases}
    1 & \text{if } t \in [t_k, t_{k+1})\\
    0 & \text{else}
  \end{cases}
\end{equation*}

A \emph{closed b-spline-function}

We choose a different weighting function based on b-splines to make use of \emph{local controllable} filters and to reduce computation significantly~\cite{Fey2017}:
\begin{equation*}
  w_k(\rho, \varphi) = \mu_k \rho \bar{N}_k^m(\varphi)
\end{equation*}
with $\boldsymbol{\mu} = (\mu_1, \ldots, \mu_K )$ learnable parameters where $\bar{N}_k^m$ is a closed b-spline-function of degree $m \in \mathbb{N}$ with uniform node vector over the interval $[0, 2\pi]$.

We can also use the Tensorproduktkonstruktion to also parametrize the distances via
\begin{equation*}
  w_{kl}(\rho, \varphi) = \mu_{kl} N_k^m(\rho) \bar{N}_l^m(\varphi)
\end{equation*}
where we combine open and closed b-spline functions.

To modify it to the 3-dimenional space, one can use the polar coordinates $(\rho, \varphi, \theta)$ to
\begin{equation*}
  w_{klq}(\rho, \varphi, \theta) = \mu_{klm} N_k^m(\rho) \bar{N}_l^m(\varphi) \bar{N}_q^m(\theta)
\end{equation*}

The good thing is the runtime.
Whereas MoNet has a runtime dependent on $K$, which is typical the dominant factor between $d$ and $K$, this method is mostly dependent on $M$, which is typical low ($1$ or $2$).









\begin{itemize}
  \item Insbesondere Gradienten beschreiben
\end{itemize}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
