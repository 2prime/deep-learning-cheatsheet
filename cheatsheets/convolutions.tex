\documentclass[pdftex,10pt,a4paper]{scrartcl}

\usepackage[a4paper,left=2.5cm,right=2.5cm,bottom=3cm,top=3cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{url}
\usepackage[numbers,sort]{natbib}
\parindent=0cm

\title{Graph Convolutions}
\date{\vspace{-5ex}}

\begin{document}

\maketitle

\section{Preliminaries}

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a \emph{weighted graph}, $\mathcal{V} = \{1, \ldots, n\}$ and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$, with weights $a_i > 0$ for each node $i \in \mathcal{V}$ and $w_{i,j} \geq 0$ for each node pair $(i, j) \in \mathcal{V} \times \mathcal{V}$, whereas $w_{i,j} > 0$ iff $(i, j) \in \mathcal{E}$.
$\mathcal{G}$ is called \emph{undirected} iff $w_{i,j} = w_{j,i}$ for all $i,j \in \mathcal{V}$.
$\mathcal{G}$ is without \emph{self-loops} iff $w_{i,i} = 0$.
Furthermore, $\mathcal{G}$ implies an \emph{adjacency matrix} $\mathbf{W} = \left( w_{i,j} \right)$, whereas $\mathbf{W}$ is mostly \emph{sparse} with $|\mathcal{E}| \ll n^2$ entries.
\\\\
The \emph{unnormalized Laplacian} of an weighted undirected graph without self-loops is a $n \times n$ symmetric positive-semidefinite matrix $\mathbf{L} = \mathbf{D} - \mathbf{W}$, where $\mathbf{D} = \mathrm{diag}\left( \sum_{i \in \mathcal{V}} w_{i,j}\right)$. (wrong syntax)
The \emph{normalized Laplacian}

$\mathcal{N}(i)$

A \emph{signal} $f \colon \mathcal{V} \to \mathbb{R}^m$ respectively $\mathbf{F} \in \mathbb{R}^{n \times m}$

Let $\mathbf{u} \colon \mathcal{V} \times \mathcal{V} \to \mathbb{R}^d$ define a $d$-dimensional vector of \emph{pseudo-coordinates}
e.g.\ the vector from $i$ to $j$ in euclidean space $\mathbf{u}(i,j) = \mathbf{p}(j) - \mathbf{p}(i)$ with \emph{graph embeddings} $\mathbf{p} \colon \mathcal{V} \to \mathbb{R}^d$
\cite{Bronstein2017}

\section{Convolutions}

\paragraph{MoNet}

MoNet~\cite{Monti2016} uses the generic \emph{patch operator} for a specific $k \in \{ 1, \ldots, K \}$
\begin{equation*}
  D_k(i)f = \sum_{j \in \mathcal{N}(i)} w_k(\mathbf{u}(i, j)) f(j)
\end{equation*}
for the spatial definition of a convolution on a graph or manifold signal $f \colon \mathcal{V} \to \mathbb{R}$ respectively $\mathbf{f} \in \mathbb{R}^n$
\begin{equation*}
  (\mathbf{f} \star \mathbf{g})(i) = \sum_k^K D_k(i)f
\end{equation*}
where $K \in \mathbb{N}$ is the dimensionality of the extracted patch and $\mathbf{w}(\mathbf{u}) \in \mathbb{R}^K$ with $\mathbf{w}(\mathbf{u}) = (w_1(\mathbf{u}), \ldots, w_K(\mathbf{u}))$ is a weighting function parametrized by some learnable parameters.
MoNet~\cite{Monti2016} suggests the use of a weighted \emph{gaussian kernel}
\begin{equation*}
  w_k(\mathbf{u}) = \exp \left(-\frac{1}{2} {(\mathbf{u} - \boldsymbol{\mu}_k)}^{\top} {\mathrm{diag}(\boldsymbol{\sigma}_k)}^{-1} (\mathbf{u} - \boldsymbol{\mu}_k) \right)
\end{equation*}

with the diagonal covariance matrix $\mathrm{diag}(\boldsymbol{\sigma}_k) \in \mathbb{R}^{d \times d}$ and mean vector $\boldsymbol{\mu}_k \in \mathbb{R}^d$, which results in $2Kd$ learnable parameters for the patch operator.

For arbitary graphs one can choose the pseudo-coordinates to use the degrees of the nodes $\mathbf{u}(i,j) = \left( \tfrac{1}{\sqrt{D_{ii}}}, \tfrac{1}{\sqrt{D_{jj}}} \right)$, whereas one can take the polar coordinates $\mathbf{u} = (\rho, \varphi)$ or $\mathbf{u} = (r, \varphi, \theta)$ for 2D respectively 3d graph embeddings like discrete manifolds.

\paragraph{SplineNet}

We choose a different weighting function based on B-Splines to make use of \emph{local controllable} filters.

\begin{equation*}
  w_k(\rho, \varphi) = \rho N_k^K(\varphi)
\end{equation*}





\begin{itemize}
  \item Erweiterung auf 3D
  \item Insbesondere Gradienten beschreiben
\end{itemize}

\begin{itemize}
  \item only parametrized by degree
\end{itemize}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
