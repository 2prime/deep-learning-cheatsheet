\documentclass[pdftex,10pt,a4paper]{scrartcl}

\usepackage[a4paper,left=2.5cm,right=2.5cm,bottom=3cm,top=3cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{url}
\usepackage{todonotes}
\usepackage[numbers,sort]{natbib}
\parindent=0cm

\title{Graph Convolutions}
\date{\vspace{-5ex}}

\begin{document}

\maketitle

\section{Preliminaries}

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{W})$ be a \emph{weighted graph} with $\mathcal{V} = \{1, \ldots, n\}$, $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ and \emph{adjacency matrix} $\mathbf{W} \in \mathbb{R}^{n \times n}$, where $W_{ij} > 0$ iff $(i, j) \in \mathcal{E}$ and $W_{ij} = 0$ iff $(i, j) \not\in \mathcal{E}$.
Note that $\mathbf{W}$ is usually \emph{sparse} with $|\mathcal{E}| \ll n^2$.
$\mathcal{G}$ is called \emph{undirected} iff $W_{ij} = W_{ji}$ for all $i,j \in \mathcal{V}$.
We further assume that $\mathcal{G}$ contains no \emph{self-loops}, meaning $(i, i) \not\in \mathcal{E}$.
$\mathbf{W}$ implies the diagonal \emph{degree matrix} $\mathbf{D} \in \mathbb{R}^{n \times n}$ with $D_{ii} = \sum_{j \in \mathcal{V}} W_{ij}$.
For a node $i \in \mathcal{V}$ its neighborhood set is defined by $\mathcal{N}(i)$.
% \\\\

% The \emph{unnormalized Laplacian} of an weighted undirected graph without self-loops is a $n \times n$ symmetric positive-semidefinite matrix $\mathbf{L} = \mathbf{D} - \mathbf{W}$, where $\mathbf{D} = \mathrm{diag}\left( \sum_{i \in \mathcal{V}} w_{ij}\right)$. (wrong syntax)
% The \emph{normalized Laplacian}

% $\mathcal{N}(i)$

% A \emph{signal} $f \colon \mathcal{V} \to \mathbb{R}^m$ respectively $\mathbf{F} \in \mathbb{R}^{n \times m}$
% Let $f \colon \mathcal{V} \to \mathbb{R}$ respectively $\mathbf{f} \in \mathbb{R}^n$ be a \emph{feature} on the nodes of the graph.


\section{Convolutions}

\paragraph{MoNet}

Let $\mathbf{u} \colon \mathcal{V} \times \mathcal{V} \to \mathbb{R}^d$ define a $d$-dimensional vector of \emph{pseudo-coordinates}.
\emph{MoNet}~\cite{Monti2016} then uses the generic \emph{patch operator}
\begin{equation*}
  D_k(i)f = \sum_{j \in \mathcal{N}(i)} w_k(\mathbf{u}(i, j)) f(j) k \in \{ 1, \ldots, K \}
\end{equation*}
for the spatial definition of a convolution on a graph or manifold signal $f \colon \mathcal{V} \to \mathbb{R}$ respectively $\mathbf{f} \in \mathbb{R}^n$
\begin{equation*}
  (\mathbf{f} \star \mathbf{g})(i) = \sum_k^K D_k(i)f
\end{equation*}
where $K \in \mathbb{N}$ is the dimensionality of the extracted patch and $\mathbf{w}(\mathbf{u}) \in \mathbb{R}^K$ with $\mathbf{w}(\mathbf{u}) = (w_1(\mathbf{u}), \ldots, w_K(\mathbf{u}))$ is a weighting function parametrized by some finite set of learnable parameters.
MoNet~\cite{Monti2016} suggests the use of the weighted \emph{gaussian kernel}
\begin{equation*}
  w_k(\mathbf{u}) = \exp \left(-\frac{1}{2} {(\mathbf{u} - \boldsymbol{\mu}_k)}^{\top} {\mathrm{diag}(\boldsymbol{\sigma}_k)}^{-1} (\mathbf{u} - \boldsymbol{\mu}_k) \right)
\end{equation*}
with the diagonal covariance matrix $\mathrm{diag}(\boldsymbol{\sigma}_k) \in \mathbb{R}^{d \times d}$ and mean vector $\boldsymbol{\mu}_k \in \mathbb{R}^d$, which results in $2Kd$ learnable parameters for the patch operator.
For arbitrary graphs one can choose the pseudo-coordinates to use the degrees of the nodes $\mathbf{u}(i,j) = \left( D_{ii}^{-1/2}, D_{jj}^{-1/2} \right)$, whereas one can take the polar coordinates $\mathbf{u} = (\rho, \varphi)$ respectively $\mathbf{u} = (r, \varphi, \theta)$ for 2D or 3d graph embeddings like manifolds.

\paragraph{SplineNet}

Let $\xi = (t_0, \ldots, t_)$ be a node vector.
An \emph{open b-spline-function} $N_k^m$ over $k \in \{ 1, \ldots, K \}$ is recursively defined by
\begin{equation*}
  N_k^0(t) = \begin{cases}
    1 & \text{if } t \in [t_k, t_{k+1})\\
    0 & \text{else}
  \end{cases}
\end{equation*}

A \emph{closed b-spline-function}

We choose a different weighting function based on \emph{B-Splines} to make use of \emph{local controllable} filters and to reduce computation significantly~\cite{Fey2017}:
\begin{equation*}
  w_k(\rho, \varphi) = \mu_k \hat{\rho} \bar{N}_k^m(\varphi)
\end{equation*}
with $\boldsymbol{\mu} = (\mu_1, \ldots, \mu_K )$ learnable parameters where $\bar{N}_k^m$ is a closed b-spline-function of degree $m \in \mathbb{N}$ with uniform node vector over the interval $[0, 2\pi]$ and $\hat{\rho} = \exp(\frac{\rho^2}{2\sigma^2})$.
\todo{emerging signal processing cite for gaussian}

We can also use the Tensorproduktkonstruktion to also parametrize the distances via\todo{lol}
\begin{equation*}
  w_{kl}(\rho, \varphi) = \mu_{kl} N_k^m(\hat{\rho}) \bar{N}_l^m(\varphi)
\end{equation*}
where we combine open and closed b-spline functions.

To modify it to the 3-dimenional space, one can use the polar coordinates $(\rho, \varphi, \theta)$ to
\begin{equation*}
  w_{klq}(\rho, \varphi, \theta) = \mu_{klm} N_k^m(\hat{\rho}) \bar{N}_l^m(\varphi) \bar{N}_q^m(\theta)
\end{equation*}

The good thing is the runtime.
Whereas MoNet has a runtime dependent on $K$, which is typical the dominant factor between $d$ and $K$, this method is mostly dependent on $M$, which is typical low ($1$ or $2$).

Computing $N_k^m(\cdot)$ for all edges and all $k \in \{ 1, \ldots, K \}$ yields $K$ adjacency matrices with $(m + 1) |\mathcal{E}|$ entries combined.

If we further restrict $m = 1$, which leads to linear interpolation, we can reach a runtime nearly independent of $K$ with $\mathcal{O}(K |\mathcal{E}| + M^{\mathrm{in}} M^{\mathrm{out}} |\mathcal{E}|)$ which is quite as fast as the GCN introduced by Kipf et al.\todo{cite}


\todo{normalize by node degree}
\todo{important: don't forget root nodes}
\todo{write tensor syntax down}

\begin{itemize}
  \item Insbesondere Gradienten beschreiben
\end{itemize}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
